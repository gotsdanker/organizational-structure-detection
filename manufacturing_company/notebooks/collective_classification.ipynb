{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from manufacturing_company.src.common.const import *\n",
    "from manufacturing_company.src.network.social_network import *\n",
    "from manufacturing_company.src.classification_algorithms.standard_classification import *\n",
    "from manufacturing_company.src.classification_algorithms.NodeInfo import NodeInfo\n",
    "from sklearn.metrics import jaccard_similarity_score, jaccard_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLLECTIVE CLASSIFICATION\n",
    "df_positions = pd.read_csv('manufacturing_company/data/raw/positions.csv', sep=';', comment='#', index_col=ID)\n",
    "\n",
    "for i in range(1, SIZE + 1):\n",
    "    df_features = pd.read_csv('manufacturing_company/data/intermediate/05_features/' + str(i) + '_months_features.csv', sep=';', index_col=ID)\n",
    "    \n",
    "    df_communication = pd.read_csv('manufacturing_company/data/intermediate/03_minimum_activity/' + str(i) + '_months_communication.csv', sep=';')\n",
    "    G = create_network(df_communication, weight=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_nodes_based_on_utility_score(utility_score_name, utility_score, pct, levels):  \n",
    "    utility_score = utility_score.sort_values(utility_score_name, ascending=False)\n",
    "\n",
    "    known_nodes_map = dict()\n",
    "    \n",
    "    for position in range(1, levels + 1):\n",
    "        employees_on_given_position = utility_score[utility_score[POSITION] == position]\n",
    "        all_nodes = len(employees_on_given_position)\n",
    "        known_nodes = round(all_nodes * pct)\n",
    "        \n",
    "        known_nodes_id = employees_on_given_position.iloc[:known_nodes].index\n",
    "        known_nodes_map.update({id: position for id in known_nodes_id})\n",
    "        \n",
    "    return known_nodes_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_passing(G, known_nodes, threshold): \n",
    "    df_nodes = pd.DataFrame(G.nodes(data='utility_score'), columns=[ID, 'utility_score'])\n",
    "    df_nodes = df_nodes.set_index(ID)\n",
    "    \n",
    "    order_desc = df_nodes.sort_values('utility_score', ascending=True).index\n",
    "    \n",
    "    nodes = pd.DataFrame(G.nodes(data='label'), columns=[ID, 'label'])\n",
    "    nodes = nodes.set_index(ID)\n",
    "    nodes = nodes.loc[order_desc, 'label'].to_dict()\n",
    "    # print(nodes)\n",
    "    label_counter = {node_id: NodeInfo() for node_id in nodes.keys()}\n",
    "    \n",
    "    G = G.to_undirected()\n",
    "    \n",
    "    end = False\n",
    "    \n",
    "    while not end:\n",
    "        old_labels = [value for (key, value) in nodes.items() if key not in known_nodes]\n",
    "        for node, label in nodes.items():\n",
    "            if label != -1:\n",
    "                neighbors = G.neighbors(node)\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor not in known_nodes:\n",
    "                        label_counter[neighbor].labels.append(label)\n",
    "        \n",
    "        for node, label in nodes.items():\n",
    "            unique_labels = len(set(label_counter[node].labels)) == 1\n",
    "            count_1 = label_counter[node].labels.count(1)\n",
    "            count_2 = round(label_counter[node].labels.count(2) / float(threshold))\n",
    "            # if label == -1:\n",
    "                # print(count_1)\n",
    "                # print(count_2)\n",
    "            # print(label_hist)\n",
    "            #     print('**************')\n",
    "            if count_1 > count_2:\n",
    "                nodes[node] = 1\n",
    "            elif count_2 > count_1:\n",
    "                nodes[node] = 2\n",
    "            elif unique_labels:\n",
    "                nodes[node] = label_counter[node].labels[0]\n",
    "            else:\n",
    "                label_counter[node].unchanged_iter += 1\n",
    "                \n",
    "            if label_counter[node].unchanged_iter > 100:\n",
    "                neighbors = G.neighbors(node)\n",
    "\n",
    "                max_utility_score = -1\n",
    "                node_label = None\n",
    "                \n",
    "                for neighbor_id in neighbors:\n",
    "                    neighbor = G.nodes[neighbor_id]\n",
    "                    if neighbor['utility_score'] > max_utility_score:\n",
    "                        max_utility_score = neighbor['utility_score']\n",
    "                        node_label = neighbor['label']\n",
    "                        \n",
    "                nodes[node] = node_label\n",
    "                label_counter[node].unchanged_iter = 0\n",
    "                \n",
    "            label_counter[node].labels = []\n",
    "            \n",
    "        new_labels = [value for (key, value) in nodes.items() if key not in known_nodes]\n",
    "        # print(old_labels)\n",
    "        # print(new_labels)\n",
    "        # print(list(nodes.values()).count(-1))\n",
    "        if (jaccard_score(old_labels, new_labels, average='micro') >= 0.99) & (-1 not in nodes.values()):\n",
    "            end = True\n",
    "        \n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collective_classification(G, df_features, pct, levels, df_positions, threshold):\n",
    "    feature_names = df_features.columns\n",
    "    \n",
    "    df_features = assign_management_levels(levels, df_features, df_positions)\n",
    "    \n",
    "    for utility_score_name in feature_names:\n",
    "        utility_score = df_features[[utility_score_name, POSITION]]     \n",
    "        known_nodes = select_nodes_based_on_utility_score(utility_score_name, utility_score, pct, levels)\n",
    "        \n",
    "        nx.set_node_attributes(G, -1, 'label')\n",
    "        nx.set_node_attributes(G, known_nodes, 'label')\n",
    "        nx.set_node_attributes(G, utility_score[utility_score_name], 'utility_score')\n",
    "        \n",
    "        nodes = message_passing(G, known_nodes, threshold)\n",
    "        # print(nodes)\n",
    "        nodes = pd.DataFrame.from_dict(nodes, orient='index', columns=[POSITION])\n",
    "        nodes.index.name = ID\n",
    "        \n",
    "        nodes = nodes.loc[~nodes.index.isin(known_nodes)]\n",
    "        \n",
    "        df_merged = pd.merge(nodes, df_features[POSITION], on=ID)\n",
    "        # print(df_merged)\n",
    "        f1 = f1_score(df_merged.iloc[:, 0], df_merged.iloc[:, 1], average='macro')\n",
    "        print('PCT: ', pct)\n",
    "        print('F1: ', f1)\n",
    "        print('Utiliti score: ', utility_score_name)\n",
    "        print('***********************************')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCT:  0.6\nF1:  0.6651162790697674\nUtiliti score:  in_degree\n***********************************\nPCT:  0.6\nF1:  0.5909090909090909\nUtiliti score:  out_degree\n***********************************\nPCT:  0.6\nF1:  0.5909090909090909\nUtiliti score:  betweenness\n***********************************\nPCT:  0.6\nF1:  0.4606741573033708\nUtiliti score:  closeness\n***********************************\nPCT:  0.6\nF1:  0.4606741573033708\nUtiliti score:  eigenvector\n***********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCT:  0.6\nF1:  0.1272727272727273\nUtiliti score:  clustering_coeff\n***********************************\nPCT:  0.6\nF1:  0.7491289198606272\nUtiliti score:  pagerank\n***********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCT:  0.6\nF1:  0.4606741573033708\nUtiliti score:  hubs\n***********************************\nPCT:  0.6\nF1:  0.619047619047619\nUtiliti score:  authorities\n***********************************\nPCT:  0.6\nF1:  0.5909090909090909\nUtiliti score:  max_clique\n***********************************\nPCT:  0.6\nF1:  0.5909090909090909\nUtiliti score:  cliques_count\n***********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCT:  0.6\nF1:  0.4606741573033708\nUtiliti score:  overtime\n***********************************\nPCT:  0.6\nF1:  0.5909090909090909\nUtiliti score:  work_at_weekend\n***********************************\nPCT:  0.6\nF1:  0.4606741573033708\nUtiliti score:  neighborhood_variability_sender\n***********************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCT:  0.6\nF1:  0.1272727272727273\nUtiliti score:  neighborhood_variability_recipient\n***********************************\nPCT:  0.6\nF1:  0.4606741573033708\nUtiliti score:  neighborhood_variability_all\n***********************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n/Users/mateusz/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "collective_classification(G, df_features, 0.6, 2, df_positions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{80: 1, 10: 1, 50: 2, 40: 2, 90: 2}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_nodes_based_on_utility_score(BETWEENNESS, df, pct, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ID: [10,20,30,40,50,60,70,80,90,100], BETWEENNESS: [5,6,2,8,9,4,6,7,8,3], POSITION: [1,2,1,2,2,2,2,1,2,1]})\n",
    "df = df.set_index(ID)\n",
    "df_positions = pd.DataFrame({ID: [10,20,30,40,50,60,70,80,90,100], POSITION: [1,2,1,2,2,2,2,1,2,1]})\n",
    "pct = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, {80: 1, 10: 1, 50: 2, 40: 2, 90: 2}, 'asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(G.nodes(data='utility_score'), columns=[ID, 'utility_score'], index=ID).sort_values('utility_score', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.round(2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[81,\n 136,\n 156,\n 62,\n 16,\n 157,\n 116,\n 140,\n 28,\n 95,\n 17,\n 97,\n 132,\n 61,\n 121,\n 123,\n 119,\n 101,\n 85,\n 89,\n 152,\n 130,\n 155,\n 47,\n 166,\n 103,\n 133,\n 73,\n 8,\n 128,\n 112,\n 27,\n 83,\n 22,\n 108,\n 58,\n 52,\n 32,\n 3,\n 113,\n 150,\n 104,\n 78,\n 94,\n 74,\n 36,\n 45,\n 56,\n 50,\n 77,\n 64,\n 14,\n 37,\n 129,\n 105,\n 102,\n 144,\n 18,\n 66,\n 148,\n 137,\n 161,\n 60,\n 54,\n 91,\n 82,\n 86,\n 141,\n 69,\n 13,\n 19,\n 33,\n 107,\n 163,\n 98,\n 67,\n 135]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(G.neighbors(39))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6453193150025995\n"
     ]
    }
   ],
   "source": [
    "neighbors = G.neighbors(77)\n",
    "\n",
    "max_utility_score = -1\n",
    "label = None\n",
    "\n",
    "for neighbor in neighbors:\n",
    "    node = G.nodes[neighbor]\n",
    "    if node['utility_score'] > max_utility_score:\n",
    "        max_utility_score = node['utility_score']\n",
    "        label = node['label']\n",
    "        \n",
    "print(label, max_utility_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1, 'utility_score': 0.6084552042160738}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
